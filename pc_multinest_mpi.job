#!/bin/bash

# --- Start SBATCH Directives ---
# Get user environment variables
#SBATCH --export=ALL

# Partition (this is the queue your job will be added to) 
#SBATCH -p batch

# Number of nodes
#SBATCH -N 2

# Number of MPI processes (tasks) per node
#SBATCH --ntasks-per-node=8

# Number of OMP threads (CPUs) per task
#SBATCH --cpus-per-task=4

# Each node contains 32 "cores". Hence
#   ntasks-per-node * cpus-per-task = 32
# Total number of used cores is N * 32

# Time allocation, which has the format (D-HH:MM:SS)
#SBATCH --time=3-00:00:00

#SBATCH --comment="N:2, MPI:8, OMP:4"

# Memory pool for all cores, per node (32 cores per node)
#SBATCH --mem=16GB
# => 512MB of memory per core

# Job Name
#SBATCH -J pc_multinest_mpi_full_no_err

# Email on following conditions
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=FAIL
#SBATCH --mail-type=END
#SBATCH --mail-user=a1648400@adelaide.edu.au
# --- End SBATCH Directives ---

export OMP_NUM_THREADS=4

# Run the job from directory in which sbatch command was run
cd $SLURM_SUBMIT_DIR

# Run pc_multinest_mpi
# ppr stands for processes per resource
# pe stands for processing elements. Used to bind a number of processing elements to each process
# Total number of processes np must be N * ntasks-per-node
mpirun -np 16 --map-by ppr:8:node ./pc_multinest_mpi output/pc_multinest_mpi_full_no_err-